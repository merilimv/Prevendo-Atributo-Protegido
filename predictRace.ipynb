{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prevendo Atributo Protegido de Atributos não Protegidos\n",
    "\n",
    "Ao conhecer a ideia de Fairness e dar os primeiros passos nessa área a primeira resolução mais óbvia dos problemas é:\n",
    "\n",
    "Retirar o Atributo Protegido deve garantir a Igualdade, já que o Classificador vai deixar de classificar de acordo com esse Atributo, então não haverá mais problemas.\n",
    "\n",
    "No entanto esse pequeno e rápido código é para demonstrar que apenas isso não resolve o problema.\n",
    "\n",
    "Isso ocorre por conta da correlação que outros atributos tem com esse Atributo Protegido. Meu objetivo é, com essa demonstração rápida, enxergar essa relação e demonstrar como um classificador pode, facilmente prever esse atributo e que, devido a esse problema, devemos buscar alternativas mais viáveis e condizentes, para buscar uma solução que resolve o problema verdadeiramente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   age              48842 non-null  int64 \n",
      " 1   workclass        48842 non-null  object\n",
      " 2   fnlwgt           48842 non-null  int64 \n",
      " 3   education        48842 non-null  object\n",
      " 4   educational-num  48842 non-null  int64 \n",
      " 5   marital-status   48842 non-null  object\n",
      " 6   occupation       48842 non-null  object\n",
      " 7   relationship     48842 non-null  object\n",
      " 8   race             48842 non-null  object\n",
      " 9   gender           48842 non-null  object\n",
      " 10  capital-gain     48842 non-null  int64 \n",
      " 11  capital-loss     48842 non-null  int64 \n",
      " 12  hours-per-week   48842 non-null  int64 \n",
      " 13  native-country   48842 non-null  object\n",
      " 14  income           48842 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\".\\\\adult.csv\")\n",
    "data.info()\n",
    "data = data.drop((data[data[\"native-country\"] == \"Holand-Netherlands\"]).index)\n",
    "# Fiz esse drop porque originalmente só tem 1 registro com esse native-country, ficava dando problema se ele não fosse pro treino então preferi retirá-lo\n",
    "\n",
    "data1 = data.drop(data[(data['race'] != 'White') & (data['race'] != \"Black\")].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Há 3 formas de testar isso, sendo essas 3 formas MUITO parecidas:\n",
    "#### Utilizando um classificador simples\n",
    "Classificar em 2 classes, sendo elas \"White\" e \"Black\", ignorando os outras classes presentes;\n",
    "\n",
    "Classificar em 2 classes, sendo elas \"White\" e \"Other\" referindo-se a todas as outras classes presentes;\n",
    "\n",
    "Classificar em 4 classes, as raças já presentes no Dataset normalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = data1.drop([\"income\", \"race\"], axis = 1)\n",
    "# Não vou utilizar \"income\" que é originalmente o atributo que desejamos prever, procuro mostrar a relação apenas entre os Atributos não Protegidos de X com o Atributo Protegido\n",
    "Y1 = data1[\"race\"]\n",
    "\n",
    "x2 = data.drop([\"income\", \"race\"], axis = 1)\n",
    "Y2 = data[\"race\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train1, x_test1, Y_train1, Y_test1 = train_test_split(x1, Y1, test_size = 0.3)\n",
    "x_train2, x_test2, Y_train2, Y_test2 = train_test_split(x2, Y2, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isso eu crio duas instâncias de x e Y, sendo a instância 1 os valores originais no entanto possuindo apenas as raças \"White\" e \"Black\" e a instâncias 2 todos os valores inicias, posteriormente a instância 2 vai virar outro Classificador já que a codificação para as classes vão ser diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categoricalLabels1 = (x_train1.select_dtypes(include = ['object'])).columns\n",
    "categoricalLabels2 = (x_train2.select_dtypes(include = ['object'])).columns\n",
    "\n",
    "encoder = OrdinalEncoder(dtype = 'int32')\n",
    "encoder2 = OrdinalEncoder(dtype = 'int32')\n",
    "encoder3 = OrdinalEncoder(dtype = 'int32')\n",
    "\n",
    "num_x_train1 = x_train1.copy()\n",
    "num_x_train2 = x_train2.copy()\n",
    "\n",
    "num_x_test1 = x_test1.copy()\n",
    "num_x_test2 = x_test2.copy()\n",
    "\n",
    "Y_test3 = Y_test2.copy()\n",
    "Y_train3 = Y_train2.copy()\n",
    "\n",
    "num_x_train1[categoricalLabels1] = encoder.fit_transform(x_train1[categoricalLabels1])\n",
    "num_x_test1[categoricalLabels1] = encoder.transform(x_test1[categoricalLabels1])\n",
    "\n",
    "num_x_train2[categoricalLabels2] = encoder2.fit_transform(x_train2[categoricalLabels2])\n",
    "num_x_test2[categoricalLabels2] = encoder2.transform(x_test2[categoricalLabels2])\n",
    "\n",
    "num_Y_test1 = Y_test1.replace({\"White\": 0, \"Black\": 1, \"Asian-Pac-Islander\": 2, \"Amer-Indian-Eskimo\": 3, \"Other\": 4})\n",
    "num_Y_train1 = Y_train1.replace({\"White\": 0, \"Black\": 1, \"Asian-Pac-Islander\": 2, \"Amer-Indian-Eskimo\": 3, \"Other\": 4})\n",
    "\n",
    "num_Y_test2 = Y_test2.replace({\"White\": 0, \"Black\": 1, \"Asian-Pac-Islander\": 1, \"Amer-Indian-Eskimo\": 1, \"Other\": 1})\n",
    "num_Y_train2 = Y_train2.replace({\"White\": 0, \"Black\": 1, \"Asian-Pac-Islander\": 1, \"Amer-Indian-Eskimo\": 1, \"Other\": 1})\n",
    "\n",
    "num_Y_test3 = Y_test3.replace({\"White\": 0, \"Black\": 1, \"Asian-Pac-Islander\": 2, \"Amer-Indian-Eskimo\": 3, \"Other\": 4})\n",
    "num_Y_train3 = Y_train3.replace({\"White\": 0, \"Black\": 1, \"Asian-Pac-Islander\": 2, \"Amer-Indian-Eskimo\": 3, \"Other\": 4})\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador da Raça de uma Pessoa\n",
    "Após fazer os tratamentos necessários, crio uma Árvore de Decisão para exibir os resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia para a Forma 1(2 classes com exclusão de dados):  0.1702310894215588\n",
      "Acurácia para a Forma 2(2 classes sem exclusão de dados):  0.19975431652221387\n",
      "Acurácia para a Forma 3(5 classes sem exclusão de dados):  0.3192520303009623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "modelo1 = DecisionTreeClassifier()\n",
    "modelo2 = DecisionTreeClassifier()\n",
    "modelo3 = DecisionTreeClassifier()\n",
    "\n",
    "modelo1.fit(num_x_train1, num_Y_train1)\n",
    "previsao1 = modelo1.predict(num_x_test1)\n",
    "\n",
    "modelo2.fit(num_x_train2, num_Y_train2)\n",
    "previsao2 = modelo2.predict(num_x_test2)\n",
    "\n",
    "modelo3.fit(num_x_train2, num_Y_train3)\n",
    "# Aqui eu posso utilizar o num_x_train2 e o num_x_test2 pois os testes 2 e 3 são exatamente iguais em x e só diferem em Y na forma da codificação das classes\n",
    "# sendo a segunda forma apenas 2 classes 0 e 1, e a terceira 5 classes 0, 1, 2, 3 e 4\n",
    "previsao3 = modelo3.predict(num_x_test2)\n",
    "\n",
    "acuraciaNP = mean_absolute_error(num_Y_test1, previsao1)\n",
    "print(\"Acurácia para a Forma 1(2 classes com exclusão de dados): \",acuraciaNP)\n",
    "acuraciaNP = mean_absolute_error(num_Y_test2, previsao2)\n",
    "print(\"Acurácia para a Forma 2(2 classes sem exclusão de dados): \",acuraciaNP)\n",
    "acuraciaNP = mean_absolute_error(num_Y_test3, previsao3)\n",
    "print(\"Acurácia para a Forma 3(5 classes sem exclusão de dados): \",acuraciaNP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados dos 3 métodos foram:\n",
    "\n",
    "1 - O erro é, em média, 0.16, variando pouco mais de 0.1 \n",
    "\n",
    "2 - Erro em torno de 0.19 a 0.20\n",
    "\n",
    "3 - Erro um pouco mais alto 0.31 variando entre 0.30 e 0.32, o que é compreensível dado ao número maior de classes e a pouca quantidade de dados para as classes minoritárias\n",
    "\n",
    "Com isso, consigo enxergar que existe sim, uma correlação entre esses dados, já que, mesmo num classificador trivial, esse erro não foi tão alto nos 2 primeiros métodos, que nesse caso são mais razoáveis devido a quantidade de classes e quantidade de dados, assim é possível entender que retirar os Dados Sensíveis não é extremamente eficiente quanto parece. \n",
    "\n",
    "Em Classificadores melhor construídos, com técnicas mais avançadas esse erro poderia ser bem menor, mas como eu busco apenas uma demonstração rápida dessa relação é suficiente para mim esses resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41f24c3b72f316e3b61812b25f63d5b07991f4c8138cc9e53a0e82c5f9f37da6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
